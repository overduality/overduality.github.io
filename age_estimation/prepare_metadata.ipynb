{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db330c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¦· DENTAL AGE ESTIMATION - DATASET PREPARATION PIPELINE\n",
      "================================================================================\n",
      "ğŸ” Loading patient metadata...\n",
      "ğŸ“‹ Columns found: ['Name', 'Date Pic Taken', 'DOB', 'Age (Decimal)', 'Ambiguous', 'Gender', 'Complete']\n",
      "ğŸ“Š Total patients in CSV: 483\n",
      "\n",
      "ğŸ“‹ Sample patient data:\n",
      "                      Name         DOB  Age (Decimal) Gender\n",
      "0             Aazkirana. F  01.07.2011           6.74      F\n",
      "1              ABDULLAH TN  07.11.2010           6.89      M\n",
      "2  ACHMAD NAUFAL AFANDI Ñƒ.  03.01.2007          10.05      M\n",
      "3               ACHMAD NUR  25.05.2007          10.00      M\n",
      "4   ACHMAD YUSUF MUAFIQ AN  24.04.2008           8.80      M\n",
      "\n",
      "ğŸ” Scanning cropped images...\n",
      "ğŸ“‚ Found 1912 image files\n",
      "\n",
      "âš ï¸  956 images could not be matched to patient data:\n",
      "   - 7_54_ORLIN_AQILA_ATALIA_26082021_mandible_right.png\n",
      "   - 11_12_IN_AMELINDA_PUTRI_F_10072019_mandible_right.png\n",
      "   - 12_16_Dinar_Nindytasari_11052018_mandible_right.png\n",
      "   - 12_57_SILFIANA_DASIFA_16102018_mandible_left.png\n",
      "   - 11_94_RAHMA_NABILA_281212017_mandible_left.png\n",
      "   - 8_48_NADINA_MONICA_SEWOW_AN_21022020_mandible_left.png\n",
      "   - 15_58_ASTI_ANANDA_NN_17062019_mandible_right.png\n",
      "   - 7_91_CALLULA_ZELDA_16072018_mandible_left.png\n",
      "   - 8_30_KANIA_CI_30072018_mandible_right.png\n",
      "   - 14_34_AZZAHRA_24022017_mandible_right.png\n",
      "   ... and 946 more\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š DATASET SUMMARY\n",
      "============================================================\n",
      "âœ… Matched images: 0\n",
      "âŒ Unmatched images: 956\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'patient_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 447\u001b[0m\n\u001b[1;32m    438\u001b[0m preparator \u001b[38;5;241m=\u001b[39m DentalDatasetPreparator(\n\u001b[1;32m    439\u001b[0m     crop_dir\u001b[38;5;241m=\u001b[39mCROP_DIR,\n\u001b[1;32m    440\u001b[0m     patient_csv\u001b[38;5;241m=\u001b[39mPATIENT_CSV,\n\u001b[1;32m    441\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mTEST_SIZE,\n\u001b[1;32m    442\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE\n\u001b[1;32m    443\u001b[0m )\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Create dataset with CSV-only splits (recommended)\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# Set create_physical_folders=True if you need separate train/test folders\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m df_final \u001b[38;5;241m=\u001b[39m \u001b[43mpreparator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_complete_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_physical_folders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True to copy files\u001b[39;49;00m\n\u001b[1;32m    450\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_final \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Dataset ready!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 390\u001b[0m, in \u001b[0;36mDentalDatasetPreparator.run_complete_pipeline\u001b[0;34m(self, output_dir, create_physical_folders)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# Step 1: Load and match data\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_match_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ ERROR: No matched data found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 117\u001b[0m, in \u001b[0;36mDentalDatasetPreparator.load_and_match_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Matched images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ Unmatched images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unmatched)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ‘¥ Unique patients: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatient_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m images_per_patient \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ“· Images per patient distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/ENTER/envs/pano/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Downloads/ENTER/envs/pano/lib/python3.10/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'patient_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "\n",
    "class DentalDatasetPreparator:\n",
    "    def __init__(self, crop_dir='cropped_obb_hemijaws_v2', \n",
    "                 patient_csv='cleaned_patient.csv',\n",
    "                 test_size=0.20, random_state=42):\n",
    "        self.crop_dir = Path(crop_dir)\n",
    "        self.patient_csv = patient_csv\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def normalize_name(self, name):\n",
    "        \"\"\"Normalize patient name for matching\"\"\"\n",
    "        if pd.isna(name):\n",
    "            return \"\"\n",
    "        # Convert to lowercase, remove extra spaces, keep only alphanumeric\n",
    "        name = str(name).lower().strip()\n",
    "        name = re.sub(r'[^a-z0-9\\s]', '', name)\n",
    "        name = re.sub(r'\\s+', ' ', name)\n",
    "        return name\n",
    "    \n",
    "    def load_and_match_data(self):\n",
    "        \"\"\"Match cropped images with patient metadata\"\"\"\n",
    "        print(\"ğŸ” Loading patient metadata...\")\n",
    "        patient_df = pd.read_csv(self.patient_csv)\n",
    "        \n",
    "        # Clean column names (remove extra spaces)\n",
    "        patient_df.columns = patient_df.columns.str.strip()\n",
    "        \n",
    "        print(f\"ğŸ“‹ Columns found: {list(patient_df.columns)}\")\n",
    "        print(f\"ğŸ“Š Total patients in CSV: {len(patient_df)}\")\n",
    "        \n",
    "        # Create normalized name column for matching\n",
    "        patient_df['normalized_name'] = patient_df['Name'].apply(self.normalize_name)\n",
    "        \n",
    "        # Show sample data\n",
    "        print(\"\\nğŸ“‹ Sample patient data:\")\n",
    "        print(patient_df[['Name', 'DOB', 'Age (Decimal)', 'Gender']].head())\n",
    "        \n",
    "        print(\"\\nğŸ” Scanning cropped images...\")\n",
    "        crop_files = list(self.crop_dir.glob('*.png'))\n",
    "        print(f\"ğŸ“‚ Found {len(crop_files)} image files\")\n",
    "        \n",
    "        dataset = []\n",
    "        unmatched = []\n",
    "        \n",
    "        for crop_file in crop_files:\n",
    "            # Skip CLAHE or raw suffix versions if they exist\n",
    "            if '_clahe.png' in crop_file.name or '_raw.png' in crop_file.name:\n",
    "                continue\n",
    "            \n",
    "            # Parse filename to extract patient name and side\n",
    "            if 'mandible_left' in crop_file.name:\n",
    "                side = 'left'\n",
    "                patient_name = crop_file.name.replace('_mandible_left.png', '')\n",
    "            elif 'mandible_right' in crop_file.name:\n",
    "                side = 'right'\n",
    "                patient_name = crop_file.name.replace('_mandible_right.png', '')\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Normalize the extracted name\n",
    "            normalized_patient = self.normalize_name(patient_name)\n",
    "            \n",
    "            # Try to find match in metadata\n",
    "            match = patient_df[patient_df['normalized_name'] == normalized_patient]\n",
    "            \n",
    "            if len(match) > 0:\n",
    "                row = match.iloc[0]\n",
    "                \n",
    "                # Get age - handle column name variations\n",
    "                age = row.get('Age (Decimal)', row.get('Age', None))\n",
    "                if pd.isna(age):\n",
    "                    print(f\"âš ï¸  Skipping {crop_file.name}: No age data\")\n",
    "                    continue\n",
    "                \n",
    "                # Get gender\n",
    "                gender = row.get('Gender', 'U')  # U for Unknown if missing\n",
    "                if pd.isna(gender):\n",
    "                    gender = 'U'\n",
    "                \n",
    "                dataset.append({\n",
    "                    'patient_id': row['Name'],  # Use original name as ID\n",
    "                    'filename': crop_file.name,\n",
    "                    'filepath': str(crop_file),\n",
    "                    'age': float(age),\n",
    "                    'gender': gender,\n",
    "                    'side': side,\n",
    "                    'dob': row.get('DOB', ''),\n",
    "                    'complete': row.get('Complete', '')\n",
    "                })\n",
    "            else:\n",
    "                unmatched.append(crop_file.name)\n",
    "        \n",
    "        df = pd.DataFrame(dataset)\n",
    "        \n",
    "        # Report unmatched images\n",
    "        if unmatched:\n",
    "            print(f\"\\nâš ï¸  {len(unmatched)} images could not be matched to patient data:\")\n",
    "            for fname in unmatched[:10]:  # Show first 10\n",
    "                print(f\"   - {fname}\")\n",
    "            if len(unmatched) > 10:\n",
    "                print(f\"   ... and {len(unmatched) - 10} more\")\n",
    "        \n",
    "        # Sanity checks\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š DATASET SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"âœ… Matched images: {len(df)}\")\n",
    "        print(f\"âŒ Unmatched images: {len(unmatched)}\")\n",
    "        print(f\"ğŸ‘¥ Unique patients: {df['patient_id'].nunique()}\")\n",
    "        \n",
    "        images_per_patient = df['patient_id'].value_counts()\n",
    "        print(f\"\\nğŸ“· Images per patient distribution:\")\n",
    "        print(images_per_patient.value_counts().sort_index())\n",
    "        \n",
    "        # Age distribution\n",
    "        print(\"\\nğŸ‚ Age distribution:\")\n",
    "        print(f\"   Range: {df['age'].min():.1f} - {df['age'].max():.1f} years\")\n",
    "        print(f\"   Mean: {df['age'].mean():.2f} Â± {df['age'].std():.2f} years\")\n",
    "        print(f\"   Median: {df['age'].median():.2f} years\")\n",
    "        \n",
    "        age_bins = pd.cut(df['age'], bins=[0, 6, 9, 12, 20], \n",
    "                         labels=['3-6', '6-9', '9-12', '12+'])\n",
    "        print(\"\\n   By age group:\")\n",
    "        print(df.groupby(age_bins, observed=True).agg({\n",
    "            'filename': 'count',\n",
    "            'age': ['mean', 'std']\n",
    "        }).round(2))\n",
    "        \n",
    "        # Gender distribution\n",
    "        print(\"\\nâš§ Gender distribution:\")\n",
    "        print(df['gender'].value_counts())\n",
    "        \n",
    "        # Side distribution\n",
    "        print(\"\\nâ†”ï¸  Side distribution:\")\n",
    "        print(df['side'].value_counts())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_age_bins(self, df, n_bins=4):\n",
    "        \"\"\"Create stratification bins for age\"\"\"\n",
    "        # Use quantile-based binning for even distribution\n",
    "        df['age_bin'] = pd.qcut(df['age'], q=n_bins, labels=False, duplicates='drop')\n",
    "        \n",
    "        print(\"\\nğŸ“¦ Age bins created for stratification:\")\n",
    "        for bin_id in sorted(df['age_bin'].unique()):\n",
    "            bin_data = df[df['age_bin'] == bin_id]\n",
    "            print(f\"  Bin {bin_id}: {bin_data['age'].min():.1f}-{bin_data['age'].max():.1f} years \"\n",
    "                  f\"({len(bin_data)} images, {bin_data['patient_id'].nunique()} patients)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def stratified_patient_split(self, df):\n",
    "        \"\"\"\n",
    "        Split patients (not images) into train/test with stratification\n",
    "        \n",
    "        Ensures:\n",
    "        1. No patient appears in both train and test\n",
    "        2. Balanced age distribution across splits\n",
    "        3. Both left and right hemijaws stay together\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ”ª SPLITTING INTO TRAIN/TEST SETS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Get patient-level statistics\n",
    "        patient_stats = df.groupby('patient_id').agg({\n",
    "            'age': 'first',  # Age is same for all images of a patient\n",
    "            'age_bin': 'first',\n",
    "            'gender': 'first',\n",
    "            'filename': 'count'  # Number of images per patient\n",
    "        }).rename(columns={'filename': 'n_images'})\n",
    "        \n",
    "        patient_stats = patient_stats.reset_index()\n",
    "        \n",
    "        print(f\"ğŸ‘¥ Total patients: {len(patient_stats)}\")\n",
    "        print(f\"ğŸ¯ Target test size: {int(len(patient_stats) * self.test_size)} patients ({self.test_size*100:.0f}%)\")\n",
    "        \n",
    "        # Stratified split by age bins\n",
    "        train_patients, test_patients = train_test_split(\n",
    "            patient_stats['patient_id'],\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state,\n",
    "            stratify=patient_stats['age_bin']\n",
    "        )\n",
    "        \n",
    "        # Create train/test dataframes\n",
    "        df_train = df[df['patient_id'].isin(train_patients)].copy()\n",
    "        df_test = df[df['patient_id'].isin(test_patients)].copy()\n",
    "        \n",
    "        # Add split column\n",
    "        df_train['split'] = 'train'\n",
    "        df_test['split'] = 'test'\n",
    "        \n",
    "        # Combine back\n",
    "        df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "        \n",
    "        # Validation checks\n",
    "        print(\"\\nâœ… VALIDATION CHECKS:\")\n",
    "        \n",
    "        # Check 1: No patient overlap\n",
    "        overlap = set(train_patients) & set(test_patients)\n",
    "        if len(overlap) == 0:\n",
    "            print(\"âœ… No patient overlap between train/test\")\n",
    "        else:\n",
    "            print(f\"âŒ WARNING: {len(overlap)} patients appear in both splits!\")\n",
    "        \n",
    "        # Check 2: Age distribution\n",
    "        print(\"\\nğŸ“Š Age distribution comparison:\")\n",
    "        print(\"\\nğŸ‹ï¸ TRAIN SET:\")\n",
    "        train_age_dist = df_train.groupby('age_bin').agg({\n",
    "            'filename': 'count',\n",
    "            'age': ['mean', 'std']\n",
    "        }).round(2)\n",
    "        print(train_age_dist)\n",
    "        \n",
    "        print(\"\\nğŸ§ª TEST SET:\")\n",
    "        test_age_dist = df_test.groupby('age_bin').agg({\n",
    "            'filename': 'count', \n",
    "            'age': ['mean', 'std']\n",
    "        }).round(2)\n",
    "        print(test_age_dist)\n",
    "        \n",
    "        # Check 3: Sample sizes\n",
    "        print(f\"\\nğŸ“ˆ Split summary:\")\n",
    "        print(f\"ğŸ‹ï¸ Train: {len(df_train)} images, {df_train['patient_id'].nunique()} patients \"\n",
    "              f\"({len(df_train)/len(df_full)*100:.1f}%)\")\n",
    "        print(f\"ğŸ§ª Test:  {len(df_test)} images, {df_test['patient_id'].nunique()} patients \"\n",
    "              f\"({len(df_test)/len(df_full)*100:.1f}%)\")\n",
    "        \n",
    "        return df_full, train_patients, test_patients\n",
    "    \n",
    "    def save_splits(self, df_full, train_patients, test_patients, output_dir='dataset_splits'):\n",
    "        \"\"\"Save train/test splits to disk\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        # Save full dataset with split labels\n",
    "        csv_path = output_path / 'full_dataset_with_splits.csv'\n",
    "        df_full.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nğŸ’¾ Saved: {csv_path}\")\n",
    "        \n",
    "        # Save separate train/test CSVs\n",
    "        df_train = df_full[df_full['split'] == 'train']\n",
    "        df_test = df_full[df_full['split'] == 'test']\n",
    "        \n",
    "        df_train.to_csv(output_path / 'train_set.csv', index=False)\n",
    "        df_test.to_csv(output_path / 'test_set.csv', index=False)\n",
    "        print(f\"ğŸ’¾ Saved: {output_path / 'train_set.csv'}\")\n",
    "        print(f\"ğŸ’¾ Saved: {output_path / 'test_set.csv'}\")\n",
    "        \n",
    "        # Save patient ID lists\n",
    "        split_info = {\n",
    "            'train_patients': sorted(train_patients.tolist()),\n",
    "            'test_patients': sorted(test_patients.tolist()),\n",
    "            'random_state': self.random_state,\n",
    "            'test_size': self.test_size,\n",
    "            'n_train_patients': len(train_patients),\n",
    "            'n_test_patients': len(test_patients),\n",
    "            'n_train_images': len(df_train),\n",
    "            'n_test_images': len(df_test),\n",
    "            'age_stats_train': {\n",
    "                'mean': float(df_train['age'].mean()),\n",
    "                'std': float(df_train['age'].std()),\n",
    "                'min': float(df_train['age'].min()),\n",
    "                'max': float(df_train['age'].max())\n",
    "            },\n",
    "            'age_stats_test': {\n",
    "                'mean': float(df_test['age'].mean()),\n",
    "                'std': float(df_test['age'].std()),\n",
    "                'min': float(df_test['age'].min()),\n",
    "                'max': float(df_test['age'].max())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(output_path / 'split_info.json', 'w') as f:\n",
    "            json.dump(split_info, f, indent=2)\n",
    "        print(f\"ğŸ’¾ Saved: {output_path / 'split_info.json'}\")\n",
    "        \n",
    "        return df_full\n",
    "    \n",
    "    def create_physical_split_folders(self, df_full, output_dir):\n",
    "        \"\"\"\n",
    "        OPTIONAL: Create separate folders with train/test images\n",
    "        \n",
    "        WARNING: This duplicates images. Use only if needed for specific training frameworks.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“ CREATING PHYSICAL TRAIN/TEST FOLDERS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for split in ['train', 'test']:\n",
    "            split_dir = Path(output_dir) / split\n",
    "            split_dir.mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "            split_df = df_full[df_full['split'] == split]\n",
    "            \n",
    "            print(f\"\\nğŸ“‚ Copying {len(split_df)} images to {split}/\")\n",
    "            \n",
    "            copied = 0\n",
    "            for _, row in split_df.iterrows():\n",
    "                src = Path(row['filepath'])\n",
    "                dst = split_dir / row['filename']\n",
    "                \n",
    "                if src.exists():\n",
    "                    shutil.copy2(src, dst)\n",
    "                    copied += 1\n",
    "            \n",
    "            print(f\"âœ… Copied {copied} files\")\n",
    "        \n",
    "        print(\"\\nâœ… Physical folders created!\")\n",
    "        print(f\"   ğŸ“ {output_dir}/train/\")\n",
    "        print(f\"   ğŸ“ {output_dir}/test/\")\n",
    "    \n",
    "    def generate_summary_report(self, df_full, output_dir='dataset_splits'):\n",
    "        \"\"\"Generate a detailed summary report\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        \n",
    "        report = []\n",
    "        report.append(\"=\"*80)\n",
    "        report.append(\"DENTAL AGE ESTIMATION DATASET - SPLIT REPORT\")\n",
    "        report.append(\"=\"*80)\n",
    "        report.append(f\"Generated: {pd.Timestamp.now()}\")\n",
    "        report.append(f\"Random seed: {self.random_state}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        for split in ['train', 'test']:\n",
    "            split_df = df_full[df_full['split'] == split]\n",
    "            \n",
    "            report.append(f\"\\n{'='*80}\")\n",
    "            report.append(f\"{split.upper()} SET\")\n",
    "            report.append(f\"{'='*80}\")\n",
    "            report.append(f\"Images: {len(split_df)}\")\n",
    "            report.append(f\"Patients: {split_df['patient_id'].nunique()}\")\n",
    "            report.append(f\"Age range: {split_df['age'].min():.2f} - {split_df['age'].max():.2f} years\")\n",
    "            report.append(f\"Mean age: {split_df['age'].mean():.2f} Â± {split_df['age'].std():.2f} years\")\n",
    "            report.append(f\"Median age: {split_df['age'].median():.2f} years\")\n",
    "            \n",
    "            report.append(f\"\\nAge distribution by bin:\")\n",
    "            age_dist = split_df.groupby('age_bin').agg({\n",
    "                'filename': 'count',\n",
    "                'age': ['mean', 'std']\n",
    "            }).round(2)\n",
    "            report.append(str(age_dist))\n",
    "            \n",
    "            report.append(f\"\\nGender distribution:\")\n",
    "            report.append(str(split_df['gender'].value_counts()))\n",
    "            \n",
    "            report.append(f\"\\nSide distribution:\")\n",
    "            report.append(str(split_df['side'].value_counts()))\n",
    "            \n",
    "            report.append(f\"\\nImages per patient:\")\n",
    "            img_per_patient = split_df.groupby('patient_id').size()\n",
    "            report.append(f\"  Min: {img_per_patient.min()}\")\n",
    "            report.append(f\"  Max: {img_per_patient.max()}\")\n",
    "            report.append(f\"  Mean: {img_per_patient.mean():.2f}\")\n",
    "        \n",
    "        # Save report\n",
    "        report_text = \"\\n\".join(report)\n",
    "        with open(output_path / 'split_report.txt', 'w') as f:\n",
    "            f.write(report_text)\n",
    "        \n",
    "        print(f\"\\nğŸ“„ Saved: {output_path / 'split_report.txt'}\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SUMMARY REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        print(report_text)\n",
    "    \n",
    "    def run_complete_pipeline(self, output_dir='dataset_splits', \n",
    "                             create_physical_folders=False):\n",
    "        \"\"\"\n",
    "        Execute complete dataset preparation pipeline\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Where to save split files\n",
    "            create_physical_folders: If True, physically copy images to train/test folders\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"ğŸ¦· DENTAL AGE ESTIMATION - DATASET PREPARATION PIPELINE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Step 1: Load and match data\n",
    "        df = self.load_and_match_data()\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(\"âŒ ERROR: No matched data found!\")\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Create age bins for stratification\n",
    "        df = self.create_age_bins(df, n_bins=4)\n",
    "        \n",
    "        # Step 3: Split patients into train/test\n",
    "        df_full, train_patients, test_patients = self.stratified_patient_split(df)\n",
    "        \n",
    "        # Step 4: Save splits\n",
    "        self.save_splits(df_full, train_patients, test_patients, output_dir)\n",
    "        \n",
    "        # Step 5: Create physical folders if requested\n",
    "        if create_physical_folders:\n",
    "            self.create_physical_split_folders(df_full, output_dir)\n",
    "        \n",
    "        # Step 6: Generate summary report\n",
    "        self.generate_summary_report(df_full, output_dir)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"âœ… DATASET PREPARATION COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"ğŸ“ All files saved to: {output_dir}/\")\n",
    "        print(\"\\nğŸ“‹ Next steps:\")\n",
    "        print(\"   1. Review split_report.txt for data distribution\")\n",
    "        print(\"   2. Use train_set.csv for model training with StratifiedGroupKFold\")\n",
    "        print(\"   3. ğŸ”’ LOCK test_set.csv - do NOT use until final evaluation\")\n",
    "        print(\"   4. Consider data augmentation for the training set\")\n",
    "        \n",
    "        return df_full\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ============================================\n",
    "    # CONFIGURATION\n",
    "    # ============================================\n",
    "    CROP_DIR = 'cropped_obb_hemijaws_v2'  # Your cropped images folder\n",
    "    PATIENT_CSV = 'cleaned_patient.csv'    # Your patient metadata CSV\n",
    "    OUTPUT_DIR = 'dataset_splits_2'\n",
    "    TEST_SIZE = 0.20  # 20% of patients for test set\n",
    "    RANDOM_STATE = 42  # For reproducibility\n",
    "    \n",
    "    # ============================================\n",
    "    # RUN PIPELINE\n",
    "    # ============================================\n",
    "    preparator = DentalDatasetPreparator(\n",
    "        crop_dir=CROP_DIR,\n",
    "        patient_csv=PATIENT_CSV,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Create dataset with CSV-only splits (recommended)\n",
    "    # Set create_physical_folders=True if you need separate train/test folders\n",
    "    df_final = preparator.run_complete_pipeline(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        create_physical_folders=False  # Set to True to copy files\n",
    "    )\n",
    "    \n",
    "    if df_final is not None:\n",
    "        print(f\"\\nâœ… Dataset ready!\")\n",
    "        print(f\"ğŸ“Š Total: {len(df_final)} images from {df_final['patient_id'].nunique()} patients\")\n",
    "        print(f\"\\nğŸ‹ï¸ Training: Use '{OUTPUT_DIR}/train_set.csv'\")\n",
    "        print(f\"ğŸ§ª Testing: Keep '{OUTPUT_DIR}/test_set.csv' locked until final evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pano",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
